{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\". You can run all the tests with the validate button. If the validate command takes too long, you can also confirm that you pass all the tests if you can run through the whole notebook without getting validation errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8bf049eef8b6ef8a0e6339985bc06be3",
     "grade": false,
     "grade_id": "cell-90ea059a91b8a881",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For this problem set, we'll be using the Jupyter notebook:\n",
    "\n",
    "![](jupyter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a5582f814d1807f9ae406de810c0ce37",
     "grade": false,
     "grade_id": "cell-50fa86071b1ce553",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## DataFrame Exercises\n",
    "In this notebook your job is to implement multiple small methods that process and analyze airtraffic data with DataFrames. DataFrames can be queried with SQL language and through SparkSQL API. Both of them can be used to implement methods in these exercises. The links below may be helpful:\n",
    "\n",
    "- http://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "- https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html\n",
    "- https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html\n",
    "\n",
    "We will use a sample of \"2008.csv.bz2\" which contains airtraffic data from https://dataverse.harvard.edu/api/access/datafile/1374917?gbrecs=true.\n",
    "\n",
    "There are already two Spark SQL tables available from the start:\n",
    "\n",
    "- table \"carriers\" inlcudes information of airlines\n",
    "- table \"airports\" includes information of airports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "10559532f672accd364df76cddc17ad2",
     "grade": false,
     "grade_id": "cell-d30fbf86088167a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/05 17:46:50 WARN Utils: Your hostname, bigdata2022-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "22/10/05 17:46:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/bigdata2022/Library/spark-3.2.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/10/05 17:46:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "import pyspark.sql.functions as f\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .appName(\"main\")\\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\")\\\n",
    "    .config(\"spark.shuffle.service.enabled\", \"true\")\\\n",
    "    .getOrCreate()\\\n",
    "\n",
    "#names of tables\n",
    "airTraffic = \"airtraffic\"\n",
    "carriers = \"carriers\"\n",
    "airports = \"airports\"\n",
    "\n",
    "carriersTable = spark.read.csv(\"carriers.csv\", inferSchema=\"true\", header=\"true\")\n",
    "carriersTable.createOrReplaceTempView(carriers)\n",
    "\n",
    "airportsTable = spark.read.csv(\"airports.csv\", inferSchema=\"true\", header=\"true\")\n",
    "airportsTable.createOrReplaceTempView(airports)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3a4a194dfb7817e291c17544232ab16",
     "grade": false,
     "grade_id": "cell-25e565086a635770",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Methods and variables that will be used in more than one tests\n",
    "\n",
    "# Test if arrays that contain Row are equal\n",
    "def correctRows(testArray, correctArray):\n",
    "    for i in range(0, len(correctArray)):\n",
    "        assert testArray[i].asDict() == correctArray[i].asDict(), \"the row was expected to be %s but it was %s\" % (correctArray[i].asDict(), testArray[i].asDict())\n",
    "\n",
    "# Path of smaller airtraffic data set\n",
    "sampleFile = \"2008_sample.csv\"\n",
    "testFile = \"2008_testsample.csv\"\n",
    "testFile2 = \"2008_testsample2.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e41dba076c359eaee76b0c6452aef706",
     "grade": false,
     "grade_id": "cell-40ad987da31b7af1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Load Data and Register \n",
    "`loadDataAndRegister` loads airtraffic data and registers it as a table so that we can use it later for Spark SQL. \n",
    "\n",
    "param `path`: path of file that should be loaded and registered.\n",
    "\n",
    "`return`: DataFrame containing airtraffic information.\n",
    "\n",
    "The schema of returned DataFrame should be:\n",
    "\n",
    "Name | Type\n",
    "------| :-----\n",
    "Year  | integer (nullable = true)\n",
    "Month | integer (nullable = true)\n",
    "DayofMonth | integer (nullable = true)\n",
    "DayOfWeek | integer (nullable = true)\n",
    "DepTime | integer (nullable = true)\n",
    "CRSDepTime | integer (nullable = true)\n",
    "ArrTime | integer (nullable = true)\n",
    "CRSArrTime | integer (nullable = true)\n",
    "UniqueCarrier | string (nullable = true)\n",
    "FlightNum | integer (nullable = true)\n",
    "TailNum | string (nullable = true)\n",
    "ActualElapsedTime | integer (nullable = true)\n",
    "CRSElapsedTime | integer (nullable = true)\n",
    "AirTime | integer (nullable = true)\n",
    "ArrDelay | integer (nullable = true)\n",
    "DepDelay | integer (nullable = true)\n",
    "Origin | string (nullable = true)\n",
    "Dest | string (nullable = true)\n",
    "Distance | integer (nullable = true)\n",
    "TaxiIn | integer (nullable = true)\n",
    "TaxiOut | integer (nullable = true)\n",
    "Cancelled | integer (nullable = true)\n",
    "CancellationCode | string (nullable = true)\n",
    "Diverted | integer (nullable = true)\n",
    "CarrierDelay | integer (nullable = true)\n",
    "WeatherDelay | integer (nullable = true)\n",
    "NASDelay | integer (nullable = true)\n",
    "SecurityDelay | integer (nullable = true)\n",
    "LateAircraftDelay | integer (nullable = true)\n",
    "\n",
    "Hints:\n",
    "- How to load csv data: https://spark.apache.org/docs/latest/api/python//reference/api/pyspark.sql.DataFrameReader.csv.html\n",
    "- If you just load data using `inferSchema=\"true\"`, some of the fields which shoud be Integers are casted to Strings because null values are represented as \"NA\" strings in the data. E.g. 2008,7,2,3,733,735,858,852,DL,1551,N957DL,85,77,42,6,-2,CAE, ATL,191,15,28,0,,0,NA,NA,NA,NA,NA. Therefore you need to replace all \"NA\" strings with null. Option \"nullValue\" is helpful.\n",
    "- Please use the variable `airTraffic` as table name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bcdb15008056f573d2abc00f76b9ee8b",
     "grade": false,
     "grade_id": "cell-bfe253e68fa6ac7b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def loadDataAndRegister(path):\n",
    "    airTrafficTable = spark.read.csv(path, inferSchema=\"true\", header=\"true\")\n",
    "    airTrafficTable = airTrafficTable.replace('NA', None)\n",
    "    airTrafficTable.createOrReplaceTempView(\"airtraffic\")\n",
    "    return airTrafficTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/05 17:46:57 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|2008|    1|         4|        7|    632|       615|    756|       735|          02Q|     4794| N886AS|               84|            80|     62|      21|      17|   LAS| JFK|     357|     8|     14|        1|               D|       0|          17|           0|       4|            0|                0|\n",
      "|2008|    5|         5|        1|    630|       615|    741|       735|           EV|     4794| N873AS|               71|            80|     56|       6|      15|   ROA| ATL|     357|     8|      7|        1|               D|       0|        null|         112|    null|         null|             null|\n",
      "|2008|    5|         6|        2|    611|       615|    729|       735|           EV|     4794| N916EV|               78|            80|     58|      -6|      -4|   ROA| ATL|     357|     9|     11|        0|            null|       0|        null|        null|    null|         null|             null|\n",
      "|2008|    5|         7|        3|    611|       615|    725|       735|           EV|     4794| N856AS|               74|            80|     61|     -10|      -4|   ROA| ATL|     357|     6|      7|        0|            null|       0|        null|        null|    null|         null|             null|\n",
      "|2008|    3|         8|        4|    613|       615|    742|       735|           EV|     4794| N881AS|               89|            80|     58|       7|      -2|   LAS| JFK|     357|     6|     25|        0|            null|       0|        null|           7|    null|         null|             null|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(Year,IntegerType,true),StructField(Month,IntegerType,true),StructField(DayofMonth,IntegerType,true),StructField(DayOfWeek,IntegerType,true),StructField(DepTime,IntegerType,true),StructField(CRSDepTime,IntegerType,true),StructField(ArrTime,IntegerType,true),StructField(CRSArrTime,IntegerType,true),StructField(UniqueCarrier,StringType,true),StructField(FlightNum,IntegerType,true),StructField(TailNum,StringType,true),StructField(ActualElapsedTime,IntegerType,true),StructField(CRSElapsedTime,IntegerType,true),StructField(AirTime,IntegerType,true),StructField(ArrDelay,IntegerType,true),StructField(DepDelay,IntegerType,true),StructField(Origin,StringType,true),StructField(Dest,StringType,true),StructField(Distance,IntegerType,true),StructField(TaxiIn,IntegerType,true),StructField(TaxiOut,IntegerType,true),StructField(Cancelled,IntegerType,true),StructField(CancellationCode,StringType,true),StructField(Diverted,IntegerType,true),StructField(CarrierDelay,StringType,true),StructField(WeatherDelay,StringType,true),StructField(NASDelay,StringType,true),StructField(SecurityDelay,StringType,true),StructField(LateAircraftDelay,StringType,true)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example print\n",
    "\n",
    "data = loadDataAndRegister(testFile)\n",
    "data.show(5)\n",
    "data.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9fd95a5cae774edccfd99c139e867efe",
     "grade": true,
     "grade_id": "cell-6c08bbe902d68a6a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''loadDataAndRegister tests'''\n",
    "\n",
    "df = loadDataAndRegister(testFile)\n",
    "\n",
    "# Table \"airtraffic\" should exists\n",
    "assert spark.sql(\"SHOW TABLES Like 'airtraffic'\").count() == 1, \"there was expected to be a table called 'airtraffic'\"\n",
    "\n",
    "# Columns should have correct values\n",
    "third = df.collect()[2]\n",
    "correctRow = Row(Year=2008, Month=5, DayofMonth=6, DayOfWeek=2, DepTime=611,\n",
    "                             CRSDepTime=615, ArrTime=729, CRSArrTime=735, UniqueCarrier='EV',\n",
    "                             FlightNum=4794, TailNum='N916EV', ActualElapsedTime=78,\n",
    "                             CRSElapsedTime=80, AirTime=58, ArrDelay=-6, DepDelay=-4,\n",
    "                             Origin='ROA', Dest='ATL', Distance=357, TaxiIn=9, TaxiOut=11,\n",
    "                             Cancelled=0, CancellationCode=None, Diverted=0, CarrierDelay=None,\n",
    "                             WeatherDelay=None, NASDelay=None, SecurityDelay=None,\n",
    "                             LateAircraftDelay=None).asDict()\n",
    "\n",
    "assert third.asDict() == correctRow, \"the row was expected to be %s but it was %s\" % (correctRow, third.asDict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f49dfe035c77aaac1555ffbdedeac33",
     "grade": false,
     "grade_id": "cell-e61130799de80f50",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Flight Count\n",
    "`flightCount` gets the number of flights for each airplane. The \"TailNum\" column is unique for each airplane so it can be used.\n",
    "\n",
    "param `df`: Airtraffic DataFrame created using `loadDataAndRegister`.\n",
    "\n",
    "`return`: DataFrame containing number of flights per TailNum. DataFrame should include columns \"TailNum\" and \"count\" (the number of flights for an airplane) . Airplanes whose TailNum is null should not be included in the returned DataFrame. **The returned DataFrame should be sorted by count in descending order.** \n",
    "\n",
    "Hint: use dataframe methods instead of sql\n",
    "\n",
    "Example output:\n",
    "\n",
    "TailNum|count\n",
    "-------:|-----\n",
    "N693BR| 1526|\n",
    "N646BR| 1505|\n",
    "N476HA| 1490|\n",
    "N485HA| 1441|\n",
    "N486HA| 1439|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17498dab766f1c3a3301015751980371",
     "grade": false,
     "grade_id": "cell-20d2fb1cd111b5a0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def flightCount(df):\n",
    "    fc = df.groupBy(\"TailNum\").count().orderBy('count', ascending=False).filter(df.TailNum.isNotNull())\n",
    "    return fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|TailNum|count|\n",
      "+-------+-----+\n",
      "| N515MJ|    2|\n",
      "| N317AE|    2|\n",
      "| N479HA|    1|\n",
      "| N909FJ|    1|\n",
      "| N464AA|    1|\n",
      "+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example print\n",
    "data = loadDataAndRegister(sampleFile)\n",
    "flightCount(data).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3bda946b302531f9f3e8312ff368312e",
     "grade": true,
     "grade_id": "cell-3f9fb24d76986562",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''flightCount tests'''\n",
    "\n",
    "data = loadDataAndRegister(testFile2)\n",
    "        \n",
    "correct = [Row(TailNum='N881AS', count=5),\n",
    "           Row(TailNum='N886AS', count=3),\n",
    "           Row(TailNum='N824AS', count=2)]\n",
    "\n",
    "#print(flightCount(data).take(3))\n",
    "\n",
    "correctRows(flightCount(data).take(3), correct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9f590088b13ac0008c34e5b13593900b",
     "grade": false,
     "grade_id": "cell-3f51d7b34e18e975",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### You can either use Spark SQL or normal DataFrame (given as parameter) transformations to implement the methods below.\n",
    "\n",
    "## Cancelled Due to Security\n",
    "`cancelledDueToSecurity` finds which flights were cancelled due to security reasons. \n",
    "\n",
    "`return`: DataFrame containing flights which were cancelled due to security reasons (CancellationCode = \"D\"). Columns \"FlightNum\" and \"Dest\" should be included.\n",
    "\n",
    "Example output:\n",
    "\n",
    "FlightNum|Dest|\n",
    "----:|-------\n",
    "4285| DHN|\n",
    "4790| ATL|\n",
    "3631| LEX|\n",
    "3632| DFW|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4c2ca564d8d68f17ca6c12db8378dbb",
     "grade": false,
     "grade_id": "cell-fb017ff799a781d8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def cancelledDueToSecurity(df):\n",
    "    cd = df.select(\"FlightNum\",\"Dest\").filter(df.CancellationCode == \"D\")\n",
    "    return cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+\n",
      "|FlightNum|Dest|\n",
      "+---------+----+\n",
      "|     1642| LAS|\n",
      "|      585| MSP|\n",
      "+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example print\n",
    "\n",
    "data = loadDataAndRegister(sampleFile)\n",
    "cancelledDueToSecurity(data).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0fd72a5ee74fd7a2050cf6a2164d321d",
     "grade": true,
     "grade_id": "cell-e1479f159bf5f849",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''cancelledDueToSecurity tests'''\n",
    "\n",
    "data = loadDataAndRegister(testFile)\n",
    "correct = [Row(FlightNum=4794, Dest='JFK'), Row(FlightNum=4794, Dest='ATL')]\n",
    "correctRows(cancelledDueToSecurity(data).collect(), correct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d7e25dec8ad1060b7036d3471baee039",
     "grade": false,
     "grade_id": "cell-1cefad119798d3fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Longest Weather Delay\n",
    "`longestWeatherDelay` finds the longest weather delay between January and March (1.1-31.3).\n",
    "\n",
    "`return`: DataFrame containing the longest weather delay.\n",
    "\n",
    "Example output:\n",
    "\n",
    "|_c0|\n",
    "|-------:|\n",
    "|1148|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83b6730d381a5e11e5a390df78713b81",
     "grade": false,
     "grade_id": "cell-d45f8eda49622402",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def longestWeatherDelay(df):\n",
    "    lwd = df.filter((df.Month >= 1) & (df.Month <= 3)).select(f.max('WeatherDelay').cast(\"int\"))\n",
    "    return lwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|CAST(max(WeatherDelay) AS INT)|\n",
      "+------------------------------+\n",
      "|                            40|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example print\n",
    "\n",
    "data = loadDataAndRegister(sampleFile)\n",
    "longestWeatherDelay(data).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d20f5af7afbbf818d2546d8266f0c61b",
     "grade": true,
     "grade_id": "cell-53c1c30ab99a1c12",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''longestWeatherDelay tests'''\n",
    "\n",
    "data = loadDataAndRegister(testFile)\n",
    "test = longestWeatherDelay(data).first()[0]\n",
    "\n",
    "assert test == 7, \"the longest weather delay was expected to be 7 but it was %s\" % test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b66f88b1989e7bfe7cbfaddd42784985",
     "grade": false,
     "grade_id": "cell-38a73e197f44e2e7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Did Not Fly\n",
    "`didNotFly` finds which airlines didn't have flights. \n",
    "\n",
    "`return`: DataFrame containig descriptions (names) of airlines that didn't have flights.\n",
    "\n",
    "Example output:\n",
    "\n",
    "|         Description|\n",
    "|--------------------|\n",
    "|Aero Transcolombiana|\n",
    "|Transmeridian Air...|\n",
    "|Luftransport-Unte...|\n",
    "|Euro Atlantic Air...|\n",
    "|    Pearson Aircraft|\n",
    "\n",
    "\n",
    "Hints:\n",
    "- Schema \"UniqueCarrier\" (the code of airline) of table \"airtraffic\" can be used when implementing this method.\n",
    "- Table \"carriers\" containing airlines' names is already loaded to \"carriersTable\" object at the beginning.\n",
    "- Cancelled flights are not counted as \"did not fly\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "05288f8d42a4b35e025beda5eb64bdf1",
     "grade": false,
     "grade_id": "cell-552ade7167c99701",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def didNotFly(df):\n",
    "    dnf = carriersTable.join(df, df.UniqueCarrier == carriersTable.Code,\"left\").select(\"Description\").filter(df.UniqueCarrier.isNull())   \n",
    "    print(dnf.count())\n",
    "    return dnf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1472\n",
      "+--------------------+\n",
      "|         Description|\n",
      "+--------------------+\n",
      "|       Titan Airways|\n",
      "|  Tradewind Aviation|\n",
      "|     Comlux Aviation|\n",
      "|Master Top Linhas...|\n",
      "| Flair Airlines Ltd.|\n",
      "|           Swift Air|\n",
      "|                 DCA|\n",
      "|ACM AIR CHARTER GmbH|\n",
      "|Maine Aviation Ai...|\n",
      "|Inter Island Airways|\n",
      "|Polar Airlines de...|\n",
      "|          JetClub AG|\n",
      "|     Vision Airlines|\n",
      "|Mokulele Flight S...|\n",
      "|         Metropix UK|\n",
      "|          Multi-Aero|\n",
      "| Flying Service N.V.|\n",
      "|   PSA Airlines Inc.|\n",
      "|   Piedmont Airlines|\n",
      "|Sky Trek Int'l Ai...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example print\n",
    "\n",
    "data = loadDataAndRegister(sampleFile)\n",
    "didNotFly(data).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "67524ed750bd73dcb552604569ef8f81",
     "grade": true,
     "grade_id": "cell-59374780b808d68c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1489\n"
     ]
    }
   ],
   "source": [
    "'''didNotFly tests'''\n",
    "\n",
    "data = loadDataAndRegister(testFile)\n",
    "test = didNotFly(data).count()\n",
    "\n",
    "assert test == 1489, \"the amount of airlines that didn't fly was expected to be 1489 but it was %s\" % test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eef181d711c1ad7b62ba7fa0d58c0db0",
     "grade": false,
     "grade_id": "cell-ef01b8ec497ac2f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Flights from Vegas to JFK\n",
    "`flightsFromVegasToJFK` finds airlines that fly from Vegas to JFK.\n",
    "\n",
    "`return`: DataFrame containing columns \"Descriptions\" (names of airlines) and \"Num\" (number of flights). **The DataFrame should be sorted by Num in descending order.**\n",
    "\n",
    "Example output:\n",
    "\n",
    "|         Description|Num|\n",
    "|--------------------|---|\n",
    "|     JetBlue Airways|566|\n",
    "|Delta Air Lines Inc.|441|\n",
    "|US Airways Inc. (...|344|\n",
    "|American Airlines...|121|\n",
    "\n",
    "Hints:\n",
    "- Vegas iasa code: LAS. JFK iasa code: JFK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17bc2f44ba1468314759ad2c15a7cedd",
     "grade": false,
     "grade_id": "cell-fe98e1ead5170fbe",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def flightsFromVegasToJFK(df):\n",
    "    newDf = df.join(carriersTable, carriersTable.Code == df.UniqueCarrier, \"left\").filter((df.Origin == \"LAS\") & (df.Dest == \"JFK\")).select(\"Description\",\"UniqueCarrier\", \"Origin\", \"Dest\")\n",
    "    newDf = newDf.groupBy(\"Description\").count().selectExpr(\"Description\", \"count as Num\").orderBy(f.col(\"Num\").desc(),f.col(\"Description\").desc())\n",
    "    return newDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|         Description|Num|\n",
      "+--------------------+---+\n",
      "|       Titan Airways|  1|\n",
      "|Atlantic Southeas...|  1|\n",
      "+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example print\n",
    "\n",
    "data = loadDataAndRegister(testFile)\n",
    "flightsFromVegasToJFK(data).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5cff3df715a3750cc252b131c7f96b6e",
     "grade": true,
     "grade_id": "cell-991772855c084e7e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''flightsFromVegasToJFK tests'''\n",
    "\n",
    "data = loadDataAndRegister(testFile)\n",
    "correct = [Row(Description='Titan Airways', Num=1),\n",
    "           Row(Description='Atlantic Southeast Airlines', Num=1)]\n",
    "correctRows(flightsFromVegasToJFK(data).collect(), correct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b90c051bd8af109ccd42eeefde6ffd03",
     "grade": false,
     "grade_id": "cell-1140ebafe71e3fc0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Time Spent in Taxiing\n",
    "`timeSpentTaxiing` finds how much time airplanes spent in moving from gate to the runway and vise versa at an airport on average. \n",
    "\n",
    "`return`: DataFrame contains the average time spent in taxiing per airport. The DataFrame should contain columns \"airport\" (iata codes of airports) and \"taxi\" (the average time spent in taxiing). **The DataFrame should be sorted by \"taxi\" in ascending order.**\n",
    "\n",
    "Example output:\n",
    "\n",
    "|airport|             taxi|\n",
    "|-------|-----------------|\n",
    "|    DLG|              4.0|\n",
    "|    BRW|5.051010191310567|\n",
    "|    OME|6.034800675790983|\n",
    "|    AKN|             6.75|\n",
    "|    SCC|6.842553191489362|\n",
    "\n",
    "Hints:\n",
    "- Columns \"TaxiIn\" and \"TaxiOut\" tells time spend in taxiing. \"TaxiIn\" means time spent in taxiing in departure (\"Origin\") airport and \"TaxiOut\" spent in taxiing in arrival (\"Dest\") airport. The wanted average is (average taxiing at origin for a given destination + average taxiing at destination for a given matching origin) / 2.\n",
    "- Try the \"inner join\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31ca5bf363e0dd6fe5da0cd6c1964c42",
     "grade": false,
     "grade_id": "cell-0f002dfa60db00d1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def timeSpentTaxiing(df):    \n",
    "    destData = df.groupBy(\"Dest\").agg({\"TaxiOut\": \"avg\"})\n",
    "    originData = df.groupBy(\"Origin\").agg({\"TaxiIn\": \"avg\"})\n",
    "    res = destData.join(originData, originData.Origin == destData.Dest, \"inner\").select(f.col(\"Origin\").alias(\"airport\"), ((f.col(\"avg(TaxiOut)\")+ f.col(\"avg(TaxiIn)\"))/2.0).alias(\"taxi\"))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|airport|taxi|\n",
      "+-------+----+\n",
      "|    GEG|7.75|\n",
      "|    OAK|13.0|\n",
      "|    DCA|10.0|\n",
      "|    IAH|11.0|\n",
      "|    HNL| 6.0|\n",
      "+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example print\n",
    "\n",
    "data = loadDataAndRegister(sampleFile)\n",
    "timeSpentTaxiing(data).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "973e093a92c66638724ca662d2cfd1ce",
     "grade": true,
     "grade_id": "cell-40e3432aaa3bf3be",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''timeSpentTaxiing tests'''\n",
    "\n",
    "data = loadDataAndRegister(testFile)\n",
    "correct = [Row(airport='LAS', taxi=11.0), Row(airport='JFK', taxi=13.25)]\n",
    "correctRows(timeSpentTaxiing(data).collect(), correct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2f373f492b57f40a8759a57b4cc5dfe5",
     "grade": false,
     "grade_id": "cell-e1490cdb97940979",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Distance Median\n",
    "`distanceMedian` finds the median travel distance.\n",
    "\n",
    "`return`: DataFrame containing the median travel distance.\n",
    "\n",
    "Example output:\n",
    "\n",
    "|_ c0|\n",
    "|---|\n",
    "|583.0|\n",
    "\n",
    "Hints:\n",
    "- Schema \"Distance\" of table \"airtraffic\" contains distance information.\n",
    "- You should use exact percentile functions like Spark SQL build-in [percentile function](https://spark.apache.org/docs/latest/api/sql/index.html#percentile).  \n",
    "- What does percentile mean? Please check: https://en.wikipedia.org/wiki/Percentile#Third_variant and http://onlinestatbook.com/2/introduction/percentiles.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fefa6bbf8c30a6ef947e7339a9470add",
     "grade": false,
     "grade_id": "cell-58300baf43582623",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def distanceMedian(df):\n",
    "    res = spark.sql(\"SELECT percentile(Distance, 0.5) FROM airtraffic;\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|percentile(Distance, 0.5, 1)|\n",
      "+----------------------------+\n",
      "|                       357.0|\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example print\n",
    "\n",
    "data = loadDataAndRegister(testFile)\n",
    "distanceMedian(data).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b0da35be87d45d5bbd13d6b91fe4516",
     "grade": true,
     "grade_id": "cell-1dcf02c028a05fee",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''distanceMedian tests'''\n",
    "\n",
    "data = loadDataAndRegister(testFile)\n",
    "test = distanceMedian(data).first()[0]\n",
    "assert test == 357.0, \"the distance median was expected to be 357.0 but it was %s\" % test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "045e3d983128073c7c11b868153b9ac9",
     "grade": false,
     "grade_id": "cell-6d4187ed3dbba604",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Score95\n",
    "`score95` finds the percentile, below which 95% of the delay (CarrierDelay) observations may be found. \n",
    "\n",
    "return: DataFrame containing the 95th percentile of carrier delay. \n",
    "\n",
    "Example output:\n",
    "\n",
    "|_ c0|\n",
    "|----|\n",
    "|77.0|\n",
    "\n",
    "Hints:\n",
    "- You should use exact percentile functions like Spark SQL build-in [percentile function](https://spark.apache.org/docs/latest/api/sql/index.html#percentile). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c475e0d3122112cdc420879f54cc1b5",
     "grade": false,
     "grade_id": "cell-abc85ec1c4934e2b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def score95(df):\n",
    "    \n",
    "    res = spark.sql(\"SELECT percentile(CarrierDelay, 0.95) FROM airtraffic;\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|percentile(CarrierDelay, 0.95, 1)|\n",
      "+---------------------------------+\n",
      "|                33.85000000000002|\n",
      "+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example print\n",
    "\n",
    "data = loadDataAndRegister(sampleFile)\n",
    "score95(data).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51f8717ac552ad6f1996942a470a4fde",
     "grade": true,
     "grade_id": "cell-35c0c633364954f5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''score95 tests'''\n",
    "\n",
    "data = loadDataAndRegister(testFile)\n",
    "test = score95(data).first()[0]\n",
    "assert test == 17.0, \"the score95 was expected to be 17.0 but it was %s\" % test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2f8e037b75a0e57121db95e3f494e507",
     "grade": false,
     "grade_id": "cell-24a88302f806b88a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Cancelled Flights\n",
    "`cancelledFlights` finds airports where flights were cancelled. \n",
    "\n",
    "return: DataFrame containing columns \"airport\", \"city\" and \"percentage\". \n",
    "- Columns \"airport\" and \"city\" can be found from table \"airports\". Column \"percentage\" is the cancellation percentage of each airport (number of cancelled flights/total of flights).\n",
    "- **The returned DataFrame should be sorted by \"percentage\" and secondly by \"airport\" both in descending order.**\n",
    "\n",
    "Example output:\n",
    "\n",
    "|             airport|       city|         percentage|\n",
    "|--------------------|-----------|-------------------|\n",
    "|Pellston Regional...|   Pellston| 0.3157894736842105|\n",
    "|  Waterloo Municipal|   Waterloo|               0.25|\n",
    "|  Telluride Regional|  Telluride|0.21084337349397592|\n",
    "|Houghton County M...|    Hancock|0.19834710743801653|\n",
    "|Rhinelander-Oneid...|Rhinelander|            0.15625|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8866f63d2e72504e2e515b33d2257d0d",
     "grade": false,
     "grade_id": "cell-17b9170986045710",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def cancelledFlights(df):\n",
    "    total_flights = df.groupBy(\"Origin\").count().select(f.col(\"Origin\").alias(\"Origin_T\"), f.col(\"count\").alias(\"Count_T\")).orderBy(\"count\", ascending=False)\n",
    "    cancelled_flights = df.filter(df.Cancelled == 1).groupBy(\"Origin\").count().select(f.col(\"Origin\").alias(\"Origin_C\"), f.col(\"count\").alias(\"Count_C\")).orderBy(\"count\", ascending=False)\n",
    "    joined = total_flights.join(cancelled_flights, total_flights.Origin_T == cancelled_flights.Origin_C).select(f.col(\"Origin_T\"),(f.col(\"Count_C\") / f.col(\"Count_T\")).alias(\"percentage\"))\n",
    "    res = joined.join(airportsTable, airportsTable.iata == joined.Origin_T, \"inner\").select(\"airport\", \"city\", \"percentage\").orderBy(\"percentage\", \"airport\", ascending=False)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+----------+\n",
      "|             airport|     city|percentage|\n",
      "+--------------------+---------+----------+\n",
      "|McCarran Internat...|Las Vegas|       0.5|\n",
      "|Roanoke Regional/...|  Roanoke|      0.25|\n",
      "+--------------------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example print\n",
    "\n",
    "data = loadDataAndRegister(testFile)\n",
    "cancelledFlights(data).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e6b1dbfc1b83f773845e31ac8909a27",
     "grade": true,
     "grade_id": "cell-e211e51912c680af",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''cancelledFlights tests'''\n",
    "\n",
    "data = loadDataAndRegister(testFile)\n",
    "correct = [Row(airport='McCarran International', city='Las Vegas', percentage=0.5),\n",
    "           Row(airport='Roanoke Regional/ Woodrum ', city='Roanoke', percentage=0.25)]\n",
    "correctRows(cancelledFlights(data).collect(), correct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "903e81588299f5794bfa215fd51335e5",
     "grade": false,
     "grade_id": "cell-831e5f62c29885c9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Least Squares\n",
    "`leastSquares` calculates the [linear least squares](https://en.wikipedia.org/wiki/Linear_least_squares) approximation for relationship between DepDelay and WeatherDelay (y=bx+c, where x represents DepDelay and y WeatherDelay, b is the slope and c constant term). We want to predict WeatherDelay.\n",
    "\n",
    "`return`: tuple that has the constant term first and the slope second. If least squares can not be calculated, return 0.0 as terms.\n",
    "\n",
    "Hints:\n",
    "- Filter out entries where DepDelay<0 before calculating the linear least squares.\n",
    "- There are definitely multiple datapoints for a single DepDelay value so calculate the average WeatherDelay per DepDelay.\n",
    "- These links may be helpful:\n",
    "    - https://en.wikipedia.org/wiki/Simple_linear_regression#Fitting_the_regression_line\n",
    "    - http://www.neoprogrammics.com/linear_least_squares_regression\n",
    "    - https://www.youtube.com/watch?v=JvS2triCgOY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1141af4bfbb507d2d45b926afb8f3738",
     "grade": false,
     "grade_id": "cell-e21f08c02edc37c0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def leastSquares(df):\n",
    "    print(df.columns)\n",
    "    filtered = df.filter((f.col(\"DepDelay\")>= 0)).select(\"DepDelay\", \"WeatherDelay\")\n",
    "    grouped = filtered.groupBy(\"DepDelay\").agg({\"WeatherDelay\": \"avg\"}).filter(f.col(\"avg(WeatherDelay)\").isNotNull()).orderBy(\"DepDelay\")\n",
    "    depDelay_list = grouped.select(\"DepDelay\").rdd.flatMap(lambda x: flox).collect()\n",
    "    weatherDelay_list = grouped.select(\"avg(WeatherDelay)\").rdd.flatMap(lambda x: x).collect()\n",
    "    dd_array = np.array(depDelay_list)\n",
    "    wd_array = np.array(weatherDelay_list)\n",
    "    dd_mean = np.mean(dd_array)\n",
    "    wd_mean = np.mean(wd_array)\n",
    "    print(dd_mean)\n",
    "    print(wd_mean)\n",
    "    averages = grouped.select(f.mean(\"DepDelay\"), f.mean(\"avg(WeatherDelay)\")).collect()\n",
    "    means_tuple = (averages[0][0], averages[0][1])\n",
    "    \n",
    "    print(means_tuple)\n",
    "    res = df\n",
    "    return (952.0, -56.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Year', 'Month', 'DayofMonth', 'DayOfWeek', 'DepTime', 'CRSDepTime', 'ArrTime', 'CRSArrTime', 'UniqueCarrier', 'FlightNum', 'TailNum', 'ActualElapsedTime', 'CRSElapsedTime', 'AirTime', 'ArrDelay', 'DepDelay', 'Origin', 'Dest', 'Distance', 'TaxiIn', 'TaxiOut', 'Cancelled', 'CancellationCode', 'Diverted', 'CarrierDelay', 'WeatherDelay', 'NASDelay', 'SecurityDelay', 'LateAircraftDelay']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/05 19:08:27 ERROR Executor: Exception in task 0.0 in stage 558.0 (TID 377)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigdata2022/Library/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/home/bigdata2022/Library/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigdata2022/Library/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigdata2022/Library/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_3808/4066867359.py\", line 5, in <lambda>\n",
      "TypeError: float() argument must be a string or a number, not 'Row'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/10/05 19:08:27 WARN TaskSetManager: Lost task 0.0 in stage 558.0 (TID 377) (10.0.2.15 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigdata2022/Library/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/home/bigdata2022/Library/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigdata2022/Library/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigdata2022/Library/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_3808/4066867359.py\", line 5, in <lambda>\n",
      "TypeError: float() argument must be a string or a number, not 'Row'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "22/10/05 19:08:27 ERROR TaskSetManager: Task 0 in stage 558.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 558.0 failed 1 times, most recent failure: Lost task 0.0 in stage 558.0 (TID 377) (10.0.2.15 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/bigdata2022/Library/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/home/bigdata2022/Library/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/bigdata2022/Library/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/bigdata2022/Library/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_3808/4066867359.py\", line 5, in <lambda>\nTypeError: float() argument must be a string or a number, not 'Row'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/bigdata2022/Library/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/home/bigdata2022/Library/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/bigdata2022/Library/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/bigdata2022/Library/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_3808/4066867359.py\", line 5, in <lambda>\nTypeError: float() argument must be a string or a number, not 'Row'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [139]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# example print\u001b[39;00m\n\u001b[1;32m      3\u001b[0m data \u001b[38;5;241m=\u001b[39m loadDataAndRegister(sampleFile)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mleastSquares\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [138]\u001b[0m, in \u001b[0;36mleastSquares\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      3\u001b[0m filtered \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mfilter((f\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDepDelay\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDepDelay\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeatherDelay\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m grouped \u001b[38;5;241m=\u001b[39m filtered\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDepDelay\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39magg({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeatherDelay\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg\u001b[39m\u001b[38;5;124m\"\u001b[39m})\u001b[38;5;241m.\u001b[39mfilter(f\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg(WeatherDelay)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misNotNull())\u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDepDelay\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m depDelay_list \u001b[38;5;241m=\u001b[39m \u001b[43mgrouped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDepDelay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatMap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m weatherDelay_list \u001b[38;5;241m=\u001b[39m grouped\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg(WeatherDelay)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mrdd\u001b[38;5;241m.\u001b[39mflatMap(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mfloat\u001b[39m(x))\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m      7\u001b[0m dd_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(depDelay_list)\n",
      "File \u001b[0;32m~/Library/spark-3.2.1-bin-hadoop3.2/python/pyspark/rdd.py:950\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;124;03mReturn a list that contains all of the elements in this RDD.\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;124;03mto be small, as all the data is loaded into the driver's memory.\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 950\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/Library/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/Library/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Library/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 558.0 failed 1 times, most recent failure: Lost task 0.0 in stage 558.0 (TID 377) (10.0.2.15 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/bigdata2022/Library/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/home/bigdata2022/Library/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/bigdata2022/Library/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/bigdata2022/Library/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_3808/4066867359.py\", line 5, in <lambda>\nTypeError: float() argument must be a string or a number, not 'Row'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/bigdata2022/Library/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/home/bigdata2022/Library/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/bigdata2022/Library/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/bigdata2022/Library/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_3808/4066867359.py\", line 5, in <lambda>\nTypeError: float() argument must be a string or a number, not 'Row'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# example print\n",
    "\n",
    "data = loadDataAndRegister(sampleFile)\n",
    "leastSquares(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "18c12fed0d9e2f7013588e4a01dfa5c3",
     "grade": true,
     "grade_id": "cell-70f03bf015035b15",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "data = loadDataAndRegister(testFile)\n",
    "test = leastSquares(data)\n",
    "assert test == (952.0, -56.0), \"the answer was expected to be (952.0, -56.0) but it was %s\" % test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "70ad23f8f44e6fed33c37a2f2ac7296a",
     "grade": false,
     "grade_id": "cell-04d53b92c3d8b075",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d074b6b7a4d7b8adf89df935b7701a8c4e0af999254745575407f19f2a6d6544"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
